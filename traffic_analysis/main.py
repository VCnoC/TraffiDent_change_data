# -*- coding: utf-8 -*-
"""
äº¤é€šäº‹æ•…æ•°æ®é¢„å¤„ç†ä¸»ç¨‹åº

å¯¹åº” MATLAB: main.m

åŠŸèƒ½ï¼š
1. åŠ è½½äº‹æ•…æ•°æ®å’Œæµé‡çŸ©é˜µ
2. å¯¹æ¯ä¸ªäº‹æ•…è¿›è¡Œä¼ æ„Ÿå™¨åŒ¹é…
3. æå–äº‹æ•…æ—¶åˆ»å‰åçš„æµé‡æ•°æ®
4. é‡‡æ ·å†å²åŒæœŸæ•°æ®
5. è¿›è¡Œç™¾åˆ†ä½åˆ†æ
6. è®¡ç®—äº‹æ•…å½±å“è¯„åˆ†
7. å¯¼å‡ºç»“æœåˆ° Excel æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py --data-dir ../data --output-dir ./output

    # å¤„ç†æŒ‡å®šèŒƒå›´çš„äº‹æ•…
    python main.py --start 0 --end 1000

    # å®Œæ•´å¤„ç†æ‰€æœ‰äº‹æ•…
    python main.py --data-dir ../data --full
"""

import argparse
import logging
import multiprocessing as mp
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any
import warnings
import re
import pandas as pd

# å¿½ç•¥ numpy çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=RuntimeWarning)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config import Config, get_config
from data.loader import load_incidents, load_sensors, load_traffic_matrices
from processors.sensor_matcher import SensorMatcher
from processors.traffic_extractor import TrafficExtractor
from processors.historical_sampler import HistoricalSampler
from processors.percentile_analyzer import PercentileAnalyzer
from processors.scorer import IncidentScorer
from processors.exporter import ResultExporter


def get_type_value(type_value: Any) -> str:
    """ä» Type å­—æ®µè·å–å¯ç”¨äºç›®å½•åçš„ç±»åˆ«å€¼ã€‚

    - ç©ºå€¼æˆ–æ— æ³•è§£æ â†’ "other"
    - å»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç©ºç™½æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    - è¿‡æ»¤å¸¸è§éæ³•è·¯å¾„å­—ç¬¦ï¼Œä¿è¯å¯ä½œä¸ºç›®å½•å
    - è¿”å›ç»Ÿä¸€å°å†™ï¼Œé¿å…å¤§å°å†™å¯¼è‡´çš„è¿‡æ»¤/åˆ†ç»„ä¸ä¸€è‡´
    """
    if pd.isna(type_value):
        return "other"

    value = str(type_value).strip()
    if not value:
        return "other"

    # å°†è¿ç»­ç©ºç™½æ›¿æ¢ä¸ºå•ä¸ªä¸‹åˆ’çº¿
    sanitized = re.sub(r"\s+", "_", value)
    # æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸å…è®¸çš„å­—ç¬¦
    sanitized = re.sub(r"[\\\\/:*?\"<>|]+", "_", sanitized)
    sanitized = sanitized.strip("._")

    if not sanitized:
        return "other"

    return sanitized.lower()


def group_incidents_by_type(
    incidents: pd.DataFrame,
    logger: logging.Logger
) -> Dict[str, pd.DataFrame]:
    """æŒ‰ Type_normalized åˆ—å¯¹äº‹æ•…è¿›è¡Œåˆ†ç»„ï¼ˆç¼ºå¤±æ—¶åŸºäº Type åŠ¨æ€ç”Ÿæˆï¼‰

    Args:
        incidents: äº‹æ•…æ•°æ® DataFrame
        logger: æ—¥å¿—å™¨

    Returns:
        å­—å…¸ï¼Œé”®ä¸º Type_normalized ç±»åˆ«å€¼ï¼Œå€¼ä¸ºè¯¥ç±»åˆ«çš„äº‹æ•… DataFrame
    """
    incidents = incidents.copy()

    if 'Type_normalized' not in incidents.columns:
        if 'Type' in incidents.columns:
            logger.info("æœªæ£€æµ‹åˆ° Type_normalized åˆ—ï¼ŒåŸºäº Type åˆ›å»ºè§„èŒƒåŒ–åˆ—ä¾›åˆ†ç»„ä½¿ç”¨")
            incidents['Type_normalized'] = incidents['Type'].apply(get_type_value)
        else:
            logger.warning("äº‹æ•…æ•°æ®ä¸­æ²¡æœ‰ Type åˆ—ï¼Œå°†æ‰€æœ‰äº‹æ•…å½’ä¸º other ç±»åˆ«")
            incidents['Type_normalized'] = "other"

    categories = incidents['Type_normalized'].unique()
    grouped = {}

    logger.info(f"æ£€æµ‹åˆ° {len(categories)} ä¸ª Type_normalized ç±»åˆ«:")
    for category in sorted(categories):
        category_df = incidents[incidents['Type_normalized'] == category].copy()
        grouped[category] = category_df
        logger.info(f"  - {category}: {len(category_df)} æ¡äº‹æ•…")

    return grouped


def setup_logging(log_level: str = "INFO") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—

    Args:
        log_level: æ—¥å¿—çº§åˆ«

    Returns:
        Logger å®ä¾‹
    """
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    return logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='äº¤é€šäº‹æ•…æ•°æ®é¢„å¤„ç†ç¨‹åº',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--data-dir',
        type=str,
        default='../data',
        help='æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ../data)'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='./output',
        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: ./output)'
    )

    parser.add_argument(
        '--start',
        type=int,
        default=None,
        help='èµ·å§‹äº‹æ•…ç´¢å¼• (é»˜è®¤: 0)'
    )

    parser.add_argument(
        '--end',
        type=int,
        default=None,
        help='ç»“æŸäº‹æ•…ç´¢å¼• (é»˜è®¤: å…¨éƒ¨)'
    )

    parser.add_argument(
        '--time-window',
        type=int,
        default=12,
        help='æ—¶é—´çª—å£å¤§å°ï¼ˆæ—¶é—´æ­¥æ•°ï¼Œé»˜è®¤: 12ï¼Œå³å‰åå„1å°æ—¶ï¼‰'
    )

    parser.add_argument(
        '--year',
        type=int,
        default=2023,
        help='æ•°æ®å¹´ä»½ (é»˜è®¤: 2023)'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=100,
        help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 100)'
    )

    parser.add_argument(
        '--log-level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )

    parser.add_argument(
        '--full',
        action='store_true',
        help='å¤„ç†æ‰€æœ‰äº‹æ•…ï¼ˆå¿½ç•¥ start/end å‚æ•°ï¼‰'
    )

    parser.add_argument(
        '--test',
        action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å‰10ä¸ªäº‹æ•…ï¼‰'
    )

    parser.add_argument(
        '--classify',
        action='store_true',
        help='æŒ‰ Type å­—æ®µåˆ†ç±»å¤„ç†ï¼Œæ¯ä¸ªç±»åˆ«è¾“å‡ºåˆ°å•ç‹¬æ–‡ä»¶å¤¹ï¼ˆå†…éƒ¨å…ˆè§„èŒƒåŒ–å¹¶è½¬ä¸ºå°å†™ï¼‰'
    )

    parser.add_argument(
        '--type-filter',
        type=str,
        action='append',
        help='åªå¤„ç†æŒ‡å®š Type å€¼çš„äº‹æ•… (å¯å¤šæ¬¡ä½¿ç”¨ï¼Œä¾‹å¦‚ --type-filter Hazardï¼Œå€¼ä¼šå…ˆè§„èŒƒåŒ–å¹¶è½¬ä¸ºå°å†™)'
    )

    parser.add_argument(
        '--type-exclude',
        type=str,
        action='append',
        help='æ’é™¤æŒ‡å®š Type å€¼çš„äº‹æ•… (å¯å¤šæ¬¡ä½¿ç”¨ï¼Œä¾‹å¦‚ --type-exclude Hazardï¼Œå€¼ä¼šå…ˆè§„èŒƒåŒ–å¹¶è½¬ä¸ºå°å†™)'
    )

    # ========================================
    # P0 ä¼˜åŒ–å‚æ•°ï¼šå¹¶è¡Œå¤„ç†
    # ========================================
    parser.add_argument(
        '--parallel',
        action='store_true',
        help='å¯ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†æ¨¡å¼ (P0 ä¼˜åŒ–)'
    )

    parser.add_argument(
        '--workers',
        type=int,
        default=None,
        help='å¹¶è¡Œå¤„ç†çš„ Worker æ•°é‡ (é»˜è®¤: CPUæ ¸å¿ƒæ•° - 25ï¼Œæœ€å°ä¸º 4)'
    )

    parser.add_argument(
        '--max-cpus',
        type=int,
        default=None,
        help='å®é™…å¯ç”¨çš„CPUæ ¸å¿ƒæ•°ï¼ˆç”¨äºå®¹å™¨ç¯å¢ƒï¼Œè‡ªåŠ¨è°ƒæ•´é»˜è®¤workeræ•°ï¼‰'
    )

    parser.add_argument(
        '--no-shared-memory',
        action='store_true',
        help='ç¦ç”¨å…±äº«å†…å­˜ä¼˜åŒ–ï¼ˆè°ƒè¯•ç”¨ï¼‰'
    )

    return parser.parse_args()


def process_batch(
    incidents,
    start_idx: int,
    end_idx: int,
    matcher: SensorMatcher,
    extractor: TrafficExtractor,
    sampler: HistoricalSampler,
    analyzer: PercentileAnalyzer,
    scorer: IncidentScorer,
    logger: logging.Logger
) -> tuple:
    """å¤„ç†ä¸€æ‰¹äº‹æ•…

    Args:
        incidents: äº‹æ•…æ•°æ®
        start_idx: èµ·å§‹ç´¢å¼•
        end_idx: ç»“æŸç´¢å¼•
        matcher: ä¼ æ„Ÿå™¨åŒ¹é…å™¨
        extractor: æµé‡æå–å™¨
        sampler: å†å²é‡‡æ ·å™¨
        analyzer: ç™¾åˆ†ä½åˆ†æå™¨
        scorer: è¯„åˆ†å™¨
        logger: æ—¥å¿—å™¨

    Returns:
        (incident_tables, common_tables, scores, analysis_results, errors, success_count)
    """
    incident_tables = []
    common_tables = []
    scores = []
    analysis_results = []
    errors = []
    success_count = 0

    for idx in range(start_idx, end_idx):
        try:
            incident = incidents.iloc[idx]

            # 1. ä¼ æ„Ÿå™¨åŒ¹é…
            match_result = matcher.match(incident, row_index=idx)
            if not match_result.success:
                errors.append({
                    'row_index': idx,
                    'incident_id': str(incident.get('incident_id', '')),
                    'error': 'sensor matching failed',
                    'details': '; '.join(match_result.errors)
                })
                continue

            # 2. æµé‡æ•°æ®æå–
            extraction_result = extractor.extract(match_result)
            if not extraction_result.success:
                errors.append({
                    'row_index': idx,
                    'incident_id': match_result.incident_id,
                    'error': 'data extraction failed',
                    'details': '; '.join(extraction_result.errors)
                })
                continue

            incident_tables.append(extraction_result.incident_table)
            common_tables.append(extraction_result.common_table)

            # 3. å†å²æ•°æ®é‡‡æ ·
            sensor_indices = match_result.matched_sensors['SensorNumber'].tolist()
            sampling_result = sampler.sample(incident['dt'], sensor_indices)

            # 4. ç™¾åˆ†ä½åˆ†æ
            analysis_result = analyzer.analyze(
                incident_id=match_result.incident_id,
                incident_data=extraction_result.incident_table,
                historical_samples=sampling_result.samples
            )
            # è®¾ç½®äº‹æ•…ç±»å‹ï¼ˆä» match_result.incident_info è·å–ï¼‰
            incident_type = match_result.incident_info.get('Type', 'other')
            analysis_result.incident_type = incident_type
            analysis_results.append(analysis_result)

            # 5. è¯„åˆ†è®¡ç®—
            score = scorer.score(
                incident_id=match_result.incident_id,
                row_index=idx,
                analysis_result=analysis_result,
                extraction_result=extraction_result,
                incident_type=incident_type  # ä¼ é€’äº‹æ•…ç±»å‹
            )
            scores.append(score)

            success_count += 1

        except Exception as e:
            errors.append({
                'row_index': idx,
                'incident_id': str(incidents.iloc[idx].get('incident_id', '')),
                'error': 'processing error',
                'details': str(e)
            })

    return incident_tables, common_tables, scores, analysis_results, errors, success_count


def process_category(
    category: str,
    category_incidents: pd.DataFrame,
    start_idx: int,
    end_idx: int,
    matcher: 'SensorMatcher',
    extractor: 'TrafficExtractor',
    sampler: 'HistoricalSampler',
    analyzer: 'PercentileAnalyzer',
    scorer: 'IncidentScorer',
    exporter: 'ResultExporter',
    batch_size: int,
    logger: logging.Logger
) -> tuple:
    """å¤„ç†å•ä¸ªç±»åˆ«çš„æ‰€æœ‰äº‹æ•…

    Args:
        category: ç±»åˆ«åç§°
        category_incidents: è¯¥ç±»åˆ«çš„äº‹æ•…æ•°æ®
        start_idx: èµ·å§‹ç´¢å¼•ï¼ˆåœ¨è¯¥ç±»åˆ«å†…ï¼‰
        end_idx: ç»“æŸç´¢å¼•ï¼ˆåœ¨è¯¥ç±»åˆ«å†…ï¼‰
        matcher: ä¼ æ„Ÿå™¨åŒ¹é…å™¨
        extractor: æµé‡æå–å™¨
        sampler: å†å²é‡‡æ ·å™¨
        analyzer: ç™¾åˆ†ä½åˆ†æå™¨
        scorer: è¯„åˆ†å™¨
        exporter: å¯¼å‡ºå™¨
        batch_size: æ‰¹å¤„ç†å¤§å°
        logger: æ—¥å¿—å™¨

    Returns:
        (total_success, total_errors, export_result)
    """
    all_incident_tables = []
    all_common_tables = []
    all_scores = []
    all_analysis_results = []
    all_errors = []
    total_success = 0

    num_to_process = end_idx - start_idx
    num_batches = (num_to_process + batch_size - 1) // batch_size

    logger.info(f"  å¤„ç†èŒƒå›´: {start_idx} - {end_idx} (å…± {num_to_process} ä¸ªäº‹æ•…)")

    for batch_num in range(num_batches):
        batch_start = start_idx + batch_num * batch_size
        batch_end = min(batch_start + batch_size, end_idx)

        logger.info(f"    æ‰¹æ¬¡ {batch_num + 1}/{num_batches}: "
                   f"äº‹æ•… {batch_start} - {batch_end}")

        results = process_batch(
            category_incidents, batch_start, batch_end,
            matcher, extractor, sampler, analyzer, scorer, logger
        )

        incident_tables, common_tables, scores, analysis_results, errors, success = results

        all_incident_tables.extend(incident_tables)
        all_common_tables.extend(common_tables)
        all_scores.extend(scores)
        all_analysis_results.extend(analysis_results)
        all_errors.extend(errors)
        total_success += success

        # æ‰“å°æ‰¹æ¬¡è¿›åº¦
        processed = batch_end - start_idx
        pct = processed / num_to_process * 100
        logger.info(f"      å®Œæˆ: {success}/{batch_end - batch_start} æˆåŠŸ, "
                   f"è¿›åº¦: {processed}/{num_to_process} ({pct:.1f}%)")

    # å¯¼å‡ºè¯¥ç±»åˆ«çš„ç»“æœ
    export_result = exporter.export(
        incident_tables=all_incident_tables if all_incident_tables else None,
        common_tables=all_common_tables if all_common_tables else None,
        scores=all_scores if all_scores else None,
        analysis_results=all_analysis_results if all_analysis_results else None,
        errors=all_errors if all_errors else None
    )

    return total_success, all_errors, export_result


def get_default_workers(max_workers: int = None) -> int:
    """è·å–é»˜è®¤çš„ Worker æ•°é‡

    åŸºäº CPU æ ¸å¿ƒæ•°è®¡ç®—ï¼Œé¢„ç•™ 25 æ ¸ç»™ç³»ç»Ÿå’Œ I/Oã€‚
    æœ€å°ä¸º 4ï¼Œæœ€å¤§ä¸º CPU æ ¸å¿ƒæ•° - 25ã€‚
    å¦‚æœæŒ‡å®šäº† max_workersï¼Œåˆ™ä¸è¶…è¿‡è¯¥å€¼ã€‚

    Args:
        max_workers: æœ€å¤§å…è®¸çš„ worker æ•°é‡ï¼ˆç”¨äºå®¹å™¨ç¯å¢ƒé™åˆ¶ï¼‰

    Returns:
        æ¨èçš„ Worker æ•°é‡
    """
    cpu_count = mp.cpu_count()
    # é¢„ç•™ 25 æ ¸ç»™ç³»ç»Ÿå’Œ I/Oï¼Œä½†è‡³å°‘ä¿ç•™ 4 ä¸ª worker
    workers = max(4, cpu_count - 25)

    # å¦‚æœæŒ‡å®šäº†æœ€å¤§é™åˆ¶ï¼Œä¸è¶…è¿‡è¯¥å€¼
    if max_workers:
        workers = min(workers, max_workers)

    return workers


def process_parallel(
    incidents: pd.DataFrame,
    sensors: pd.DataFrame,
    data_dir: str,
    output_dir: str,
    num_workers: int,
    time_window: int,
    year: int,
    start_idx: int,
    end_idx: int,
    exporter: 'ResultExporter',
    logger: logging.Logger
) -> tuple:
    """ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†äº‹æ•…æ•°æ®

    P0 ä¼˜åŒ–ï¼šå¤šè¿›ç¨‹å¹¶è¡Œ + å…±äº«å†…å­˜

    Args:
        incidents: äº‹æ•…æ•°æ®
        sensors: ä¼ æ„Ÿå™¨æ•°æ®
        data_dir: æ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        num_workers: Worker æ•°é‡
        time_window: æ—¶é—´çª—å£
        year: æ•°æ®å¹´ä»½
        start_idx: èµ·å§‹ç´¢å¼•
        end_idx: ç»“æŸç´¢å¼•
        exporter: å¯¼å‡ºå™¨
        logger: æ—¥å¿—å™¨

    Returns:
        (total_success, all_errors, export_result)
    """
    from parallel_processor import ParallelProcessor

    logger.info("=" * 40)
    logger.info("ğŸš€ P0 ä¼˜åŒ–æ¨¡å¼ï¼šå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†")
    logger.info("=" * 40)
    logger.info(f"  Worker æ•°é‡: {num_workers}")
    logger.info(f"  å¤„ç†èŒƒå›´: {start_idx} - {end_idx}")
    logger.info(f"  å…±äº«å†…å­˜: å¯ç”¨")

    def progress_callback(processed: int, total: int):
        pct = processed / total * 100
        logger.info(f"  è¿›åº¦: {processed}/{total} ({pct:.1f}%)")

    # åˆ›å»ºå¹¶è¡Œå¤„ç†å™¨
    processor = ParallelProcessor(
        data_dir=data_dir,
        num_workers=num_workers,
        time_window=time_window,
        year=year
    )

    try:
        # æ‰§è¡Œå¹¶è¡Œå¤„ç†
        result = processor.process_incidents(
            incidents=incidents,
            sensors=sensors,
            start_idx=start_idx,
            end_idx=end_idx,
            progress_callback=progress_callback
        )

        # å¯¼å‡ºç»“æœ
        export_result = exporter.export(
            incident_tables=result.incident_tables if result.incident_tables else None,
            common_tables=result.common_tables if result.common_tables else None,
            scores=result.scores if result.scores else None,
            analysis_results=result.analysis_results if result.analysis_results else None,
            errors=result.errors if result.errors else None
        )

        return result.total_success, result.errors, export_result

    finally:
        processor.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    logger = setup_logging(args.log_level)

    logger.info("=" * 60)
    logger.info("äº¤é€šäº‹æ•…æ•°æ®é¢„å¤„ç†ç¨‹åº")
    logger.info("=" * 60)

    # æ˜¾ç¤ºå¹¶è¡Œå¤„ç†æ¨¡å¼
    if args.parallel:
        num_workers = args.workers if args.workers else get_default_workers(args.max_cpus)
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç†æ¨¡å¼å·²å¯ç”¨ (Workers: {num_workers})")

    start_time = datetime.now()

    # è®¾ç½®è·¯å¾„
    data_dir = Path(args.data_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"æ•°æ®ç›®å½•: {data_dir}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"æ—¶é—´çª—å£: {args.time_window}")
    logger.info(f"æ•°æ®å¹´ä»½: {args.year}")

    # åŠ è½½æ•°æ®
    logger.info("-" * 40)
    logger.info("åŠ è½½æ•°æ®...")

    try:
        incidents_path = data_dir / 'incidents_y2023.csv'
        sensors_path = data_dir / 'sensor_meta_feature.csv'

        incidents = load_incidents(str(incidents_path))
        sensors = load_sensors(str(sensors_path))
        matrices = load_traffic_matrices(str(data_dir), use_mmap=True)

        logger.info(f"äº‹æ•…æ•°æ®: {len(incidents)} æ¡")
        logger.info(f"ä¼ æ„Ÿå™¨æ•°æ®: {len(sensors)} ä¸ª")
        logger.info(f"æµé‡çŸ©é˜µå½¢çŠ¶: {matrices.occupancy.shape}")

        # ========================================
        # Type å­—æ®µè§„èŒƒåŒ–ï¼ˆè§£å†³æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼‰
        # ========================================
        # åŸå› ï¼šè¿‡æ»¤æ—¶ä½¿ç”¨åŸå§‹å€¼ï¼Œä½†åˆ†ç±»æ—¶ä½¿ç”¨è§„èŒƒåŒ–å€¼ï¼Œå¯¼è‡´ä¸ä¸€è‡´
        # ä¾‹å¦‚ï¼šæ•°æ®ä¸­ "Fire  " æ— æ³•åŒ¹é… --type-filter Fire
        # è§£å†³ï¼šç»Ÿä¸€ä½¿ç”¨ get_type_value() è¿›è¡Œè§„èŒƒåŒ–
        # å˜åŒ–ï¼š "Fire  " â†’ "fire", "Car/Fire" â†’ "car_fire", "" â†’ "other"
        if 'Type' in incidents.columns:
            incidents['Type_normalized'] = incidents['Type'].apply(get_type_value)
        else:
            logger.warning("äº‹æ•…æ•°æ®ä¸­æ²¡æœ‰ Type åˆ—ï¼Œåˆ›å»º Type_normalized=otherï¼ˆå°å†™ï¼‰ä»¥ä¿è¯åç»­é€»è¾‘å¯ç”¨")
            incidents['Type_normalized'] = "other"

        normalized_categories = incidents['Type_normalized'].unique()
        logger.info(
            "Type è§„èŒƒåŒ–ï¼ˆå·²è½¬å°å†™ï¼‰å®Œæˆ: %d ä¸ªç±»åˆ« (ç¤ºä¾‹: %s)" % (
                len(normalized_categories),
                ', '.join(sorted(normalized_categories[:5])) if len(normalized_categories) > 0 else 'NONE'
            )
        )

    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return 1

    # è¿‡æ»¤ Type
    if args.type_filter:
        logger.info(f"è¿‡æ»¤ä¿ç•™ Type (è§„èŒƒåŒ–å¹¶å°å†™å): {args.type_filter}")
        if 'Type_normalized' in incidents.columns:
            normalized_filter = [get_type_value(t) for t in args.type_filter]
            original_count = len(incidents)
            incidents = incidents[incidents['Type_normalized'].isin(normalized_filter)].copy()
            filtered_count = len(incidents)
            logger.info(f"  è¿‡æ»¤ç»“æœ: {original_count} -> {filtered_count} (ç§»é™¤ {original_count - filtered_count} æ¡)")
        else:
            logger.warning("äº‹æ•…æ•°æ®ä¸­æ²¡æœ‰ Type_normalized åˆ—ï¼Œæ— æ³•æ‰§è¡Œè¿‡æ»¤")

    if args.type_exclude:
        logger.info(f"è¿‡æ»¤æ’é™¤ Type (è§„èŒƒåŒ–å¹¶å°å†™å): {args.type_exclude}")
        if 'Type_normalized' in incidents.columns:
            normalized_exclude = [get_type_value(t) for t in args.type_exclude]
            original_count = len(incidents)
            incidents = incidents[~incidents['Type_normalized'].isin(normalized_exclude)].copy()
            filtered_count = len(incidents)
            logger.info(f"  è¿‡æ»¤ç»“æœ: {original_count} -> {filtered_count} (ç§»é™¤ {original_count - filtered_count} æ¡)")
        else:
            logger.warning("äº‹æ•…æ•°æ®ä¸­æ²¡æœ‰ Type_normalized åˆ—ï¼Œæ— æ³•æ‰§è¡Œè¿‡æ»¤")

    if len(incidents) == 0:
        logger.warning("è¿‡æ»¤åæ— äº‹æ•…æ•°æ®ï¼Œé€€å‡ºç¨‹åº")
        return 0
        
    # é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿åç»­å¤„ç†é€»è¾‘æ­£ç¡®
    incidents = incidents.reset_index(drop=True)

    # ç¡®å®šå¤„ç†èŒƒå›´
    total_incidents = len(incidents)

    if args.test:
        start_idx = 0
        end_idx = min(10, total_incidents)
        logger.info("æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰10ä¸ªäº‹æ•…")
    elif args.full:
        start_idx = 0
        end_idx = total_incidents
        logger.info("å®Œæ•´æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰äº‹æ•…")
    else:
        start_idx = args.start if args.start is not None else 0
        end_idx = args.end if args.end is not None else min(start_idx + 100, total_incidents)

    logger.info(f"å¤„ç†èŒƒå›´: {start_idx} - {end_idx} (å…± {end_idx - start_idx} ä¸ªäº‹æ•…)")

    # åˆå§‹åŒ–å¤„ç†å™¨
    logger.info("-" * 40)
    logger.info("åˆå§‹åŒ–å¤„ç†å™¨...")

    matcher = SensorMatcher(sensors, matrices.node_order)
    extractor = TrafficExtractor(matrices, args.time_window, args.year)
    sampler = HistoricalSampler(matrices, args.time_window, args.year)
    analyzer = PercentileAnalyzer()
    scorer = IncidentScorer()
    exporter = ResultExporter(str(output_dir))

    # åˆ†æ‰¹å¤„ç†
    logger.info("-" * 40)
    logger.info("å¼€å§‹å¤„ç†...")

    all_incident_tables = []
    all_common_tables = []
    all_scores = []
    all_analysis_results = []
    all_errors = []
    total_success = 0

    batch_size = args.batch_size

    # ========================================
    # åˆ†ç±»å¤„ç†æ¨¡å¼ï¼ˆ--classify å¯ç”¨æ—¶ï¼‰
    # ========================================
    if args.classify:
        logger.info("=" * 40)
        logger.info("å¯ç”¨åˆ†ç±»å¤„ç†æ¨¡å¼")
        logger.info("=" * 40)

        # æŒ‰ Type åˆ†ç»„
        grouped_incidents = group_incidents_by_type(incidents, logger)

        # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
        category_stats = {}

        for category in sorted(grouped_incidents.keys()):
            category_incidents = grouped_incidents[category]
            category_total = len(category_incidents)

            # è®¡ç®—è¯¥ç±»åˆ«å†…çš„å¤„ç†èŒƒå›´
            # ä½¿ç”¨å…¨å±€ start_idx/end_idx çš„æ¯”ä¾‹æ¥ç¡®å®šç±»åˆ«å†…çš„èŒƒå›´
            if args.full:
                cat_start = 0
                cat_end = category_total
            elif args.test:
                cat_start = 0
                cat_end = min(10, category_total)
            else:
                # æŒ‰æ¯”ä¾‹è®¡ç®—æˆ–ä½¿ç”¨å…¨éƒ¨
                cat_start = 0
                cat_end = category_total

            if cat_start >= cat_end:
                logger.info(f"\nè·³è¿‡ç±»åˆ« [{category}]: æ— äº‹æ•…éœ€è¦å¤„ç†")
                continue

            logger.info(f"\n{'='*40}")
            logger.info(f"å¤„ç†ç±»åˆ«: [{category}]")
            logger.info(f"  è¯¥ç±»åˆ«å…± {category_total} æ¡äº‹æ•…")
            logger.info(f"  å¤„ç†èŒƒå›´: {cat_start} - {cat_end}")

            # åˆ›å»ºç±»åˆ«ä¸“å±çš„è¾“å‡ºç›®å½•
            category_output_dir = output_dir / category
            category_output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"  è¾“å‡ºç›®å½•: {category_output_dir}")

            # åˆ›å»ºç±»åˆ«ä¸“å±çš„å¯¼å‡ºå™¨
            category_exporter = ResultExporter(str(category_output_dir))

            # å¤„ç†è¯¥ç±»åˆ«çš„äº‹æ•…
            cat_success, cat_errors, cat_export = process_category(
                category=category,
                category_incidents=category_incidents,
                start_idx=cat_start,
                end_idx=cat_end,
                matcher=matcher,
                extractor=extractor,
                sampler=sampler,
                analyzer=analyzer,
                scorer=scorer,
                exporter=category_exporter,
                batch_size=batch_size,
                logger=logger
            )

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            category_stats[category] = {
                'total': category_total,
                'processed': cat_end - cat_start,
                'success': cat_success,
                'failed': len(cat_errors)
            }

            total_success += cat_success
            all_errors.extend(cat_errors)

            # æ‰“å°è¯¥ç±»åˆ«çš„å¯¼å‡ºæ–‡ä»¶
            for file_path in cat_export.files_created:
                logger.info(f"    å·²åˆ›å»º: {file_path}")

        # æ‰“å°åˆ†ç±»å¤„ç†æ±‡æ€»
        logger.info("\n" + "=" * 40)
        logger.info("åˆ†ç±»å¤„ç†æ±‡æ€»")
        logger.info("=" * 40)
        for category, stats in sorted(category_stats.items()):
            success_rate = stats['success'] / stats['processed'] * 100 if stats['processed'] > 0 else 0
            logger.info(f"  [{category}]: {stats['success']}/{stats['processed']} æˆåŠŸ ({success_rate:.1f}%)")

    # ========================================
    # æ™®é€šå¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    # ========================================
    else:
        # ========================================
        # P0 ä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†æ¨¡å¼
        # ========================================
        if args.parallel:
            num_workers = args.workers if args.workers else get_default_workers(args.max_cpus)

            total_success, all_errors, export_result = process_parallel(
                incidents=incidents,
                sensors=sensors,
                data_dir=str(data_dir),
                output_dir=str(output_dir),
                num_workers=num_workers,
                time_window=args.time_window,
                year=args.year,
                start_idx=start_idx,
                end_idx=end_idx,
                exporter=exporter,
                logger=logger
            )

            for file_path in export_result.files_created:
                logger.info(f"  å·²åˆ›å»º: {file_path}")

        # ========================================
        # ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        # ========================================
        else:
            num_batches = (end_idx - start_idx + batch_size - 1) // batch_size

            for batch_num in range(num_batches):
                batch_start = start_idx + batch_num * batch_size
                batch_end = min(batch_start + batch_size, end_idx)

                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{num_batches}: "
                           f"äº‹æ•… {batch_start} - {batch_end}")

                results = process_batch(
                    incidents, batch_start, batch_end,
                    matcher, extractor, sampler, analyzer, scorer, logger
                )

                incident_tables, common_tables, scores, analysis_results, errors, success = results

                all_incident_tables.extend(incident_tables)
                all_common_tables.extend(common_tables)
                all_scores.extend(scores)
                all_analysis_results.extend(analysis_results)
                all_errors.extend(errors)
                total_success += success

                # æ‰“å°æ‰¹æ¬¡è¿›åº¦
                processed = batch_end - start_idx
                total = end_idx - start_idx
                pct = processed / total * 100
                logger.info(f"  å®Œæˆ: {success}/{batch_end - batch_start} æˆåŠŸ, "
                           f"æ€»è¿›åº¦: {processed}/{total} ({pct:.1f}%)")

            # å¯¼å‡ºç»“æœï¼ˆä»…ä¸²è¡Œæ¨¡å¼éœ€è¦åœ¨è¿™é‡Œå¯¼å‡ºï¼‰
            logger.info("-" * 40)
            logger.info("å¯¼å‡ºç»“æœ...")

            export_result = exporter.export(
                incident_tables=all_incident_tables if all_incident_tables else None,
                common_tables=all_common_tables if all_common_tables else None,
                scores=all_scores if all_scores else None,
                analysis_results=all_analysis_results if all_analysis_results else None,
                errors=all_errors if all_errors else None
            )

            for file_path in export_result.files_created:
                logger.info(f"  å·²åˆ›å»º: {file_path}")

    # å¯¼å‡ºå¤„ç†æ‘˜è¦
    end_time = datetime.now()
    summary_path = exporter.export_summary(
        total_incidents=total_incidents,
        processed=end_idx - start_idx,
        successful=total_success,
        failed=len(all_errors),
        start_time=start_time,
        end_time=end_time
    )
    logger.info(f"  å·²åˆ›å»º: {summary_path}")

    # æ‰“å°æ‘˜è¦
    duration = (end_time - start_time).total_seconds()
    logger.info("-" * 40)
    logger.info("å¤„ç†å®Œæˆï¼")
    logger.info(f"  æ€»äº‹æ•…æ•°: {total_incidents}")
    logger.info(f"  å¤„ç†äº‹æ•…: {end_idx - start_idx}")
    logger.info(f"  æˆåŠŸ: {total_success}")
    logger.info(f"  å¤±è´¥: {len(all_errors)}")
    logger.info(f"  æˆåŠŸç‡: {total_success/(end_idx-start_idx)*100:.2f}%")
    logger.info(f"  è€—æ—¶: {duration:.2f} ç§’")
    logger.info(f"  å¹³å‡æ¯ä¸ªäº‹æ•…: {duration/(end_idx-start_idx):.3f} ç§’")
    logger.info("=" * 60)

    return 0


if __name__ == '__main__':
    sys.exit(main())
